# LangMARL: Natural Language Multi-Agent Reinforcement Learning

LangMARL is a **language-native multi-agent reinforcement learning (MARL) framework** that reformulates **policy representation, credit assignment, and policy optimization entirely in natural language**.
It enables large language model (LLM)‚Äìbased agents to learn cooperative behaviors under the **Centralized Training and Decentralized Execution (CTDE)** paradigm with improved interpretability, sample efficiency, and scalability.

---

## ‚ú® Core Idea

Traditional MARL relies on numeric parameters, scalar rewards, and gradient-based optimization. LangMARL introduces a paradigm shift by treating **natural language as a first-class optimization space**:

* **Policies are Language**: Each agent‚Äôs policy is represented as natural language rules or instructions
* **Credits are Language**: A centralized critic assigns agent-level credit using trajectory-level language analysis
* **Optimization is Language Evolution**: Policies are updated via language critiques instead of numeric gradients

> Natural language is not just a communication or explanation layer‚Äîit is the medium for learning itself.

---

## üß© Framework Overview

### 1. LLM Actors with Language Policies

* Each agent is instantiated as an LLM
* The policy is encoded as natural language (rules, heuristics, preferences)
* Input: textual local observation
* Output: action description (mapped to environment actions)

### 2. Centralized Language Critic

* Used only during training
* Has access to the full episode trajectory
* Produces **agent-specific, causal, and interpretable language credits**
* Explicitly explains how each agent‚Äôs behavior contributed to success or failure

### 3. Cross-Trajectory Credit Aggregation

* A summary LLM aggregates language credits from multiple Monte Carlo rollouts
* Replaces numeric expectation with semantic abstraction
* Produces stable, high-level credit signals per agent

### 4. Language Policy Optimizer

* Converts credit signals into policy critiques
* Updates policies via language revision (editing rules, adding constraints, refining priorities)

---

## üîÅ Algorithm: LangMARL (CTDE)

1. **Decentralized Execution & Experience Collection**

   * Each agent independently acts according to its own language policy
   * Full episode trajectories are collected

2. **Centralized Language-Based Credit Assignment**

   * A centralized critic analyzes full trajectories
   * Generates natural language credit for each agent

3. **Cross-Trajectory Credit Summarization**

   * A summary LLM abstracts consistent behavioral patterns across rollouts

4. **Language-Level Policy Optimization**

   * Credits are transformed into critiques
   * Each agent‚Äôs language policy is updated accordingly

At test time, **all agents execute fully decentralized**, without access to centralized components.

---

## üß™ Experimental Environments

### Overcooked-AI

* Two-agent cooperative environment
* Sparse team rewards
* Requires long-horizon coordination and role differentiation
* Hierarchical action space (semantic subgoals + low-level controls)

### Pistonball

* Large-scale cooperative control (10‚Äì20 agents)
* Severe partial observability
* Shared global reward with complex local causality
* Emphasizes scalability and credit assignment under many agents

---

## üìä Results Summary

### Baselines

* **Zero-shot LLM**: No learning or adaptation
* **TextGrad**: Treats language feedback as gradient-like signals, without explicit multi-agent credit modeling

### Key Findings

* LangMARL consistently outperforms baselines in:

  * Final performance
  * Sample efficiency
  * Training stability
  * Scalability with increasing number of agents
* Performance gains are especially large in:

  * Sparse-reward settings
  * Asymmetric-role tasks
  * Large-team coordination scenarios

---

## üîç Ablation and Analysis

* **Removing agent-wise credit assignment leads to significant performance drops**
* LangMARL is robust across different LLM backbones
* Rollout count affects stability but exhibits non-monotonic behavior
* Improvements stem primarily from **language-based credit assignment**, not raw model scale

---

## üìå Advantages of LangMARL

* ‚úÖ Interpretable, agent-level learning signals
* ‚úÖ Causal, trajectory-level credit assignment
* ‚úÖ No reliance on numeric gradients or model fine-tuning
* ‚úÖ Compatible with black-box LLMs
* ‚úÖ Effective for long-horizon, sparse-reward, and large-scale cooperative tasks
